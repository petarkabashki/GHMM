<html>
<head>
<title>GHMM: General Hidden Markov Model library: Documentation</title>
</head>
<style type="text/css" media="all">
@import "layout1.css" screen;
@import "print.css" print;
</style>
<div id="Content">


<h2>Documentation</h2>

<p>Currently, the GHMM is utterly lacking in documentation. The best
sources are a standard text on HMM such as Rabiner's Tutorial on
Hidden Markov Models to understand the theory, the publications using
the GHMM and the help information, in particular in the comments in
the Python wrapper.

Note that you can see the help on the Python prompt by typing <kbd>help(ghmm)</kbd>
after  <kbd>import ghmm</kbd>. Help also works on variables such as GHMM objects,
so if <kbd>m = ghmm.HMMFromMatrices(...</kbd>, then <kbd>help(m)</kbd> will provide
specific information about  <kbd>m</kbd>.
</p>



<h5>Further sources of documentation</h5>

<P><ul>

<li>Working example snippets for using the GHMM are provided by the unit test cases
in <kbd>ghmm/ghmmwrapper/ghmmunittests.py</kbd>.

<li>There are web-pages derived from the comments for the <A HREF="ghmm_html/index.html">C library</a>.

<li>There are web-pages derived from the comments for <A HREF="Pydoc/index.html">Python bindings</a>.
 

<li>Specific questions might have been discussed on the mailing list:
Check the <A
HREF="http://sourceforge.net/mailarchive/forum.php?forum_name=ghmm-list">Archives</a>
or subscribe <A
HREF="http://lists.sourceforge.net/lists/listinfo/ghmm-list">subscribe</a>
to the <A HREF="mailto:ghmm-list@lists.sourceforge.net">GHMM mailing
list</a>.
</ul>
</p>


<h3>A brief tutorial</h3>

<p>In the following, we assume that you have installed GHMM including
the Python bindings. Download the <A
HREF="UnfairCasino.py">UnfairCasino.py</A>-file.</p>





<p>You might have seen the unfair casino example (Biological
Sequence Analysis, Durbin et. al, 1998), where a dealer in a casino
occasionally exchanges a fair dice with a loaded one. We do not know
if and when this exchange happens, we only can observe the throws.
This stochastic process we will model with a HMM.</p>

<p>Below <kbd>></kbd> is your shell prompt and
<kbd>>>></kbd> is the prompt of the Python interpreter and
you should type whatever follows the prompt omitting the
blank. Alternatively, you can enter the commands in a text file <kbd>
foo.py</kbd> and execute that text file with <kbd>python -i foo.py</kbd>. The
<kbd>-i</kbd> option will bring up the Python prompt after executing <kbd>
foo.py</kbd> with all the variables and functions you have defined in the
file.</p>

<H4>Getting started</H4>
<p><pre><kbd>
> python
Python 2.6.4 (r264:75706, Mar 16 2010, 09:46:46) 
[GCC 4.0.1 (Apple Inc. build 5490)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import ghmm
>>> help('ghmm')
Help on module ghmm:

NAME
    ghmm - The Design of ghmm.py

...
</kbd></pre></p>

<H4>Creating the first model:</H4> 
<p>There are two states in our
example.  Either the dice is fair (state 0; Python indexes arrays like
C and C++ from 0) or it is loaded (state 1). We will define the
transition and emission matrices explicitly.</p>

<p>To save us some typing (namely <kbd>ghmm.</kbd> before any class or
function from GHMM), we will import all symbols from ghmm directly.
<pre><kbd>
>>> from ghmm import *
</kbd></pre>
Our alphabet are the numbers on the dice {1,2,3,4,5,6}.  To be
consistent with Python's range function the upper limit is <b>not</b>
part of a range.
<pre><kbd>
>>> sigma = IntegerRange(1,7)
</kbd></pre>
The transition matrix <em>A</em> is chosen such that we will stay longer in
the fair than the loaded state. The emission probabilities are set
accordingly in <em>B</em> --- guessing equal probabilities for the fair dice
and higher probabilities for ones and twos etc. for the loaded dice
--- and the initial state distribution <em>pi</em> has no preference for
either state.
<pre><kbd>
>>> A = [[0.9, 0.1], [0.3, 0.7]]
>>> efair = [1.0 / 6] * 6
>>> print efair
[0.16666666666666666, 0.16666666666666666, ... 0.16666666666666666]
>>> eloaded = [3.0 / 13, 3.0 / 13, 2.0 / 13, 2.0 / 13, 2.0 / 13, 1.0 / 13]
>>> B = [efair, eloaded]
>>> pi = [0.5] * 2
>>> m = HMMFromMatrices(sigma, DiscreteDistribution(sigma), A, B, pi)
>>> print m
</kbd></pre>
</p>

<H4>Generating sequences</H4>

<p>At this time GHMM still exposes some internals. For example, symbols
from discrete alphabets are internally represented as <em>0,...,|alphabet|
-1</em> and the alphabet is responsible for translating between internal
and external representations.</p>

<p>First we sample from the model, specifying the length of the sequence,
and then transfer the observations to the external representation.
<pre><kbd>
>>> obs_seq = m.sampleSingle(20)
>>> print obs_seq
>>> obs = map(sigma.external, obs_seq)
>>> print obs
[1, 5, 4, 1, 3, 4, 1, 1, 3, 4, 3, 5, 5, 1, 5, 2, 1, 5, 3, 5]
</kbd></pre>
</p>

 
<H4>Learning from data</H4>

<p>In a real application we would want to train the model. That is, we
would like to estimate the parameters of the model from data with the
goal to maximize the likelihood of that data given the model. This is
done for HMMs with the Baum-Welch algorithm which is actually an
instance of the well-known Expectation-Maximization procedure for
missing data problems in statistics. The process is iterative and
hence sometime called re-estimation.</p>

<p>We can use <kbd>m</kbd> as a starting point and use some 'real'
data to train it.  The variable <kbd>train_seq</kbd> is an
<kbd>EmissionSequence</kbd> or <kbd>SequenceSet</kbd> object.
<pre><kbd> 
>>> from UnfairCasino import train_seq 
>>> m.baumWelch(train_seq)
</kbd></pre></p>

<p>If you want more fine-grained control over the learning procedure, you
can do single steps and monitor the relevant diagnostics yourself, or
employ meta-heuristics such as noise-injection to avoid getting stuck
in local maxima. [Currently for continous emissions only]</p>

<H4>Computing a Viterbi-path</H4>

<p>A Viterbi-path is a sequence of states <em>Q</em> maximizing the probability
<em>P[Q|observations, model]</em>. If we compute a
Viterbi-path for a unfair casino sequence of throws, the state
sequence tells us when the casino was fair (state 0) and when it
was unfair (state 1).
<pre><kbd>
>>> from UnfairCasino import test_seq
>>> v = m.viterbi(test_seq)
>>> print v
</kbd></pre>
</p>

<p>You can investigate how successful your model is at detecting cheating.
Just create artificial observations and see if you can detect them.
<pre><kbd>
>>> my_seq = EmissionSequence(sigma, [1] * 20 + [6] * 10 + [1] * 40)
>>> print my_seq
>>> print m.viterbi(my_seq)
</kbd></pre> 
</p>




</div>

<div id="Menu">
        <a href="index.html" title="">GHMM</a><br />
        <a href="installation.html" title="">Installation</a><br />
        <a href="documentation.html" title="">Documentation</a><br />
        <a href="hmmed.html" title="">HMMEd</a><br />
        <a href="gql.html" title="">GQL</a><br />
        <a href="contributors.html" title="">Contributors</a><br />
</div>

</body>
</html>
